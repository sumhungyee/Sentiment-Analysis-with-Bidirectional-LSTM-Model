{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Sentiment Classifier\n",
    "\n",
    "This document outlines the process of crafting a machine learning model using a Long-Short-Term-Memory (LSTM) neural network. This model is trained on a dataset that contains 50000 movie reviews (Maas 2011). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing basic dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "6NgcWD1UODtf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\hungy\\anaconda3\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorflow) (2.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorflow) (15.0.6.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorflow) (1.21.5)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorflow) (1.51.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorflow) (63.4.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorflow) (4.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorflow) (23.3.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\hungy\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hungy\\anaconda3\\lib\\site-packages (3.5.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from matplotlib) (1.21.5)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hungy\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from scikit-learn) (1.9.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hungy\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: click in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hungy\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install tensorflow\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "!pip install -U nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ycZXbkGFKZmB",
    "outputId": "ef1eadd0-84f1-4509-f33c-e393d636534e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hungy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hungy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import keras \n",
    "import nltk \n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout, TimeDistributed, Bidirectional, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer # decided that Lemmatizer is better\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the Dataset\n",
    "Individual reviews are initially stored as text files. These reviews are collated and compiled into one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fO24DmziW6vN",
    "outputId": "cbf54f03-773e-46b1-a9b5-cf3d60d6849a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 reviews  Positivity\n",
      "49648  Contains SpoilersThis is a Peter Watkins film ...           1\n",
      "36485  Very good s movie about mob operations in New ...           1\n",
      "10126  I have a month old and got really tired of wat...           1\n",
      "10139  For the big thinkers among us The Intruder is ...           0\n",
      "48473  I must have been around ten years old when my ...           1\n",
      "...                                                  ...         ...\n",
      "538    Comedies often have the unfortunate reputation...           1\n",
      "27093  Its easy to see why many people consider In th...           1\n",
      "35629  Tonights film course film was The Legend of th...           0\n",
      "13077  There are laughs in this film that is for sure...           0\n",
      "49583  I could almost wish this movie had not been ma...           0\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df1 = pd.read_csv(\"trainreviews.csv\")\n",
    "df2 = pd.read_csv(\"testreviews.csv\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning Data\n",
    "The dataframe is checked for rows where nothing is entered as reviews. Fortunately, there are no empty reviews in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [reviews, Positivity]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [reviews, Positivity]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(df1[df1['reviews'].isnull()])\n",
    "print(df2[df2['reviews'].isnull()])\n",
    "df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatising Words and Further Cleaning\n",
    "The dataset contains unwanted phrases or words, such as website links, as well as html code and numbers. To reduce noise within the dataset (and to aid computation), these unwanted phrases are removed.\n",
    "Furthermore, to aid with computation, words are lemmatised. For instance, the word \"eats\" will be reduced to \"eat\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MF-dQGN8trCH",
    "outputId": "0481a5f1-7ba7-452a-9cb7-18315a90877c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 reviews  Positivity\n",
      "49648  [contains, spoilersthis, is, a, peter, watkins...           1\n",
      "36485  [very, good, s, movie, about, mob, operation, ...           1\n",
      "10126  [i, have, a, month, old, and, got, really, tir...           1\n",
      "10139  [for, the, big, thinker, among, u, the, intrud...           0\n",
      "48473  [i, must, have, been, around, ten, year, old, ...           1\n",
      "...                                                  ...         ...\n",
      "538    [comedy, often, have, the, unfortunate, reputa...           1\n",
      "27093  [it, easy, to, see, why, many, people, conside...           1\n",
      "35629  [tonight, film, course, film, wa, the, legend,...           0\n",
      "13077  [there, are, laugh, in, this, film, that, is, ...           0\n",
      "49583  [i, could, almost, wish, this, movie, had, not...           0\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#stopwds = set(stopwords.words(\"english\"))\n",
    "#stopwds.remove('not')\n",
    "\n",
    "def clean(words):\n",
    "    cleaned = re.sub(\"<.*?\\/>\", \"\", words)\n",
    "    cleaned = re.sub(\"http:[^\\s]*\\s\", \"\", cleaned) \n",
    "    cleaned = re.sub(\"[^a-zA-Z0-9\\s]+\", \"\", cleaned) # get rid of special characters\n",
    "    cleaned = re.sub(r'\\w*\\d\\w*', \"\", cleaned)\n",
    "    return cleaned\n",
    "    \n",
    "\n",
    "def lem(words):\n",
    "    words = clean(words)\n",
    "    words = words.lower().split()\n",
    "    #words = [w for w in words if not w in stopwds and len(w) >= 3]\n",
    "    #words = [w for w in words]\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    return words\n",
    "\n",
    "\n",
    "df['reviews'] = df['reviews'].map(lambda x: lem(x))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenisation\n",
    "A tokeniser is used to convert words into discrete tokens. Thereafter, with the function `fit_on_texts`, an internal \"vocabulary\" system is created over all these words in the dataset. The function `texts_to_sequences` then converts each sentence (each row of words above) into a list of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_FpJDMbZxIa2",
    "outputId": "38b57aa6-de6f-4f2d-c35f-fb1a2dc220df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input length: 228\n",
      "vocabulary: 140601\n",
      "[[ 1332 18141     6 ...    80  8093    33]\n",
      " [   51    47   148 ...     0     0     0]\n",
      " [    9    24     2 ...     0     0     0]\n",
      " ...\n",
      " [ 3915    14   270 ...   933     6  1660]\n",
      " [   36    21   314 ...     0     0     0]\n",
      " [    9    97   219 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "revls = []\n",
    "for review in df['reviews']:\n",
    "    revls.append(len(review))\n",
    "input_len = int(np.mean(revls) + 1)\n",
    "\n",
    "print(f\"input length: {input_len}\")\n",
    "    \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['reviews'])\n",
    "vocab = len(tokenizer.word_index) + 1 \n",
    "print(f\"vocabulary: {vocab}\")\n",
    "sequences = tokenizer.texts_to_sequences(df['reviews'])\n",
    "data = pad_sequences(sequences, maxlen=input_len, padding='post', truncating='post')\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above code, we learn that the average length of each review is 228-words long, and there are a total of 140601 unique tokens (including words with spelling mistakes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting into Test and Train Datasets\n",
    "To know how well our model learns, the dataset is split into two pieces (in an 80-20 ratio). The larger portion will be used to train the machine learning model, and the model will be tested against the smaller portion of the dataset. \n",
    "\n",
    "The larger dataset contains 40000 sentences while the smaller dataset contains 10000, as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    9  4453   380 ...     0     0     0]\n",
      " [    2   210    46 ...     0     0     0]\n",
      " [  204  2698   694 ...     0     0     0]\n",
      " ...\n",
      " [    9   215     1 ...     0     0     0]\n",
      " [    2  4003  2042 ...     0     0     0]\n",
      " [    1    53 93932 ...     0     0     0]]\n",
      "40000\n",
      "[[8084    1  971 ...    0    0    0]\n",
      " [  71    7    6 ... 2770  448   18]\n",
      " [ 365   79    3 ...    0    0    0]\n",
      " ...\n",
      " [  71    8    5 ...    0    0    0]\n",
      " [1987 2373  372 ...  363    1  754]\n",
      " [ 528   42   44 ...    0    0    0]]\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, df['Positivity'], test_size=0.2, stratify=df['Positivity'])\n",
    "\n",
    "print(X_train)\n",
    "print(len(X_train))\n",
    "print(X_test)\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model\n",
    "\n",
    "Using the Keras module, the LSTM model is created below. A Bidirectional LSTM model is appropriate here as LSTMs deal with sequences, which means that the order of words within a sentence will be important. For example, \"not good\" and \"good ... not...\" are treated differently. Therefore, this method is more appropriate than just adding the sentiments of each word in each sentence.\n",
    "\n",
    "Each token above is now converted into a vector of 90 dimensions. The model is first trained in small batches at a time for 20 epochs, before it is trained on the entire dataset for 4 more epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is a binary classification problem, binary cross entropy is used as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wV82pELFfIGY",
    "outputId": "e8f1614d-73b0-40cb-b185-95523369177f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM_Sentiment_Classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 228, 90)           12654090  \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 228, 360)         390240    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 228, 360)          0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 180)              324720    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 180)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                5792      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 264       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,375,115\n",
      "Trainable params: 13,375,115\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "dims = 90\n",
    "model = Sequential(name=\"LSTM_Sentiment_Classifier\")\n",
    "model.add(Embedding(vocab, dims, input_length=input_len))\n",
    "model.add(Bidirectional(LSTM(dims*2 , return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(dims)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "40000 preprocessed sentences from the larger dataframe is fed into the LSTM model for training. Notably, `val_accuracy` is the accuracy of the model against the test dataset of 10000 sentences, and `accuracy` is the accuracy of the model while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "134/134 [==============================] - 26s 133ms/step - loss: 0.5217 - accuracy: 0.7377 - val_loss: 0.4230 - val_accuracy: 0.8173\n",
      "Epoch 2/20\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 0.2688 - accuracy: 0.8955 - val_loss: 0.3076 - val_accuracy: 0.8879\n",
      "Epoch 3/20\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 0.1442 - accuracy: 0.9503 - val_loss: 0.3431 - val_accuracy: 0.8652\n",
      "Epoch 4/20\n",
      "134/134 [==============================] - 17s 126ms/step - loss: 0.0840 - accuracy: 0.9729 - val_loss: 0.4520 - val_accuracy: 0.8613\n",
      "Epoch 5/20\n",
      "134/134 [==============================] - 17s 126ms/step - loss: 0.0537 - accuracy: 0.9841 - val_loss: 0.4985 - val_accuracy: 0.8726\n",
      "Epoch 6/20\n",
      "134/134 [==============================] - 17s 128ms/step - loss: 0.0368 - accuracy: 0.9888 - val_loss: 0.5683 - val_accuracy: 0.8670\n",
      "Epoch 7/20\n",
      "134/134 [==============================] - 17s 126ms/step - loss: 0.0390 - accuracy: 0.9876 - val_loss: 0.6514 - val_accuracy: 0.8532\n",
      "Epoch 8/20\n",
      "134/134 [==============================] - 17s 124ms/step - loss: 0.0383 - accuracy: 0.9877 - val_loss: 0.5500 - val_accuracy: 0.8628\n",
      "Epoch 9/20\n",
      "134/134 [==============================] - 17s 126ms/step - loss: 0.0209 - accuracy: 0.9931 - val_loss: 0.6347 - val_accuracy: 0.8592\n",
      "Epoch 10/20\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 0.0155 - accuracy: 0.9954 - val_loss: 0.7479 - val_accuracy: 0.8654\n",
      "Epoch 11/20\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 0.0123 - accuracy: 0.9961 - val_loss: 0.7198 - val_accuracy: 0.8643\n",
      "Epoch 12/20\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 0.0139 - accuracy: 0.9956 - val_loss: 0.6430 - val_accuracy: 0.8633\n",
      "Epoch 13/20\n",
      "134/134 [==============================] - 17s 128ms/step - loss: 0.0143 - accuracy: 0.9955 - val_loss: 0.6706 - val_accuracy: 0.8543\n",
      "Epoch 14/20\n",
      "134/134 [==============================] - 17s 128ms/step - loss: 0.0094 - accuracy: 0.9972 - val_loss: 0.7914 - val_accuracy: 0.8531\n",
      "Epoch 15/20\n",
      "134/134 [==============================] - 17s 128ms/step - loss: 0.0117 - accuracy: 0.9959 - val_loss: 0.7685 - val_accuracy: 0.8615\n",
      "Epoch 16/20\n",
      "134/134 [==============================] - 17s 128ms/step - loss: 0.0061 - accuracy: 0.9982 - val_loss: 0.7966 - val_accuracy: 0.8585\n",
      "Epoch 17/20\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 0.0064 - accuracy: 0.9981 - val_loss: 0.8885 - val_accuracy: 0.8602\n",
      "Epoch 18/20\n",
      "134/134 [==============================] - 17s 128ms/step - loss: 0.0059 - accuracy: 0.9980 - val_loss: 0.8745 - val_accuracy: 0.8520\n",
      "Epoch 19/20\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 0.0090 - accuracy: 0.9973 - val_loss: 0.9003 - val_accuracy: 0.8513\n",
      "Epoch 20/20\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 0.0154 - accuracy: 0.9951 - val_loss: 0.7473 - val_accuracy: 0.8583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2277f9687f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size = 300, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "40000/40000 [==============================] - 2838s 71ms/step - loss: 0.5901 - accuracy: 0.6514 - val_loss: 0.3366 - val_accuracy: 0.8607\n",
      "Epoch 2/4\n",
      "40000/40000 [==============================] - 2779s 69ms/step - loss: 0.2660 - accuracy: 0.8950 - val_loss: 0.3189 - val_accuracy: 0.8764\n",
      "Epoch 3/4\n",
      "40000/40000 [==============================] - 3248s 81ms/step - loss: 0.1613 - accuracy: 0.9433 - val_loss: 0.3500 - val_accuracy: 0.8614\n",
      "Epoch 4/4\n",
      "40000/40000 [==============================] - 3257s 81ms/step - loss: 0.1065 - accuracy: 0.9664 - val_loss: 0.3914 - val_accuracy: 0.8786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22776a85910>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size = 1, epochs = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a final accuracy of 87.86%, the model seems to perform relatively well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model\n",
    "The model is saved for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM_Sentiment_Classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 228, 90)           12654090  \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 228, 360)         390240    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 228, 360)          0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 180)              324720    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 180)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                5792      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 264       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,375,115\n",
      "Trainable params: 13,375,115\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models\\assets\n"
     ]
    }
   ],
   "source": [
    "model_path = './models'\n",
    "exists = os.path.exists(model_path)\n",
    "if not exists:\n",
    "    os.mkdir(model_path)\n",
    "print(model.summary())\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Tests\n",
    "Some additional prompts are created and tested on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             reviews\n",
      "0  [the, romance, between, the, two, character, w...\n",
      "1  [i, enjoyed, watching, the, basketball, match,...\n",
      "[[   1  816  207    1  111   48   13  184 1967  276  333   13   78   17\n",
      "     1   12   13   19   47    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [   9  506  152    1 5092  880  266    9   96    1 1012   66  184  492\n",
      "   152    1  880   13  184 1111    3  248    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "def processor(df_test, vocab, input_len, tokenizer):\n",
    "    df_test['reviews'] = df_test['reviews'].map(lambda x: lem(x))\n",
    "    print(df_test)\n",
    "    \n",
    "    sequences = tokenizer.texts_to_sequences(df_test['reviews'])\n",
    "    data = pad_sequences(sequences, maxlen=input_len, padding ='post', truncating='post')\n",
    "    print(data)\n",
    "    return data\n",
    "\n",
    "prompt_1 = \"the romance between the two characters was quite unrealistic. \\\n",
    "everything else was great, but the movie was not good\"\n",
    "prompt_2 = \"i enjoyed watching the basketball matches. \\\n",
    "although i think the players were quite horrible, watching the matches was quite exciting and fun\"\n",
    "df_test = pd.DataFrame(np.array([[prompt_1], [prompt_2]]), columns = ['reviews'])\n",
    "data_test = processor(df_test, vocab, input_len, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "As with the labels in the original dataset, a prediction of a number close to \"1\" denotes that the sentiment of the input is likely to be positive, while a prediction of a number close to \"0\" denotes the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.12854104],\n",
       "       [0.9878555 ]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(data_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Works Cited\n",
    "Maas, Andrew, et al. \"Learning word vectors for sentiment analysis.\" _Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies_. 2011."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
